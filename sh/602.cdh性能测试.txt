!! 由于数据库中没有主键，而使用sqoop导入时必须指定一个主键，就使用了receive_time，约500万数据导入到hbase后，count时只有90万，因为有receive_time是一样的，
hbase就认为是更新数据，总数量就少了。

#单硬盘速度。
hive count 
515万 37.77秒
1519万 85秒
hive select single
515万 40.42秒


hbase count 
90万 38秒

hbase select single
90万 253毫秒

#####使用三块硬盘
hive count 
515万 32.56秒
1519万 50秒

hive select single
515万 29秒

hbase count 
90万 30秒


hbase select single
90万 174毫秒

hive 大表与一个小表join后group by 
1519万 179秒


！！测试结果
基于hdfs的hive数据，只有在select * from table [limit N ] 不带过滤条件时才直接读取数据，其余过程均要转换为mr过程执行。
hive程序在运行时只能单线程运行，第一个提交的任务没有完成前，后续的任务都会等待，可以看到hadoop上只有一个mr过程在运行。
hive的表没有索引，除了直读数据的，都会全表扫描。所以对一个表的count 、 group 、 where 带条件所用的时间都差不多。
（hive目前的版本已经支持索引，但与普通数据库的索引不同，不能自动更新，需要使用前手动更新，如果是大量数据还可以提高速度，少量数据反而会增加计算步骤，使用更多的时间）
hive有分区和分桶的功能，在表的数据量较大的情况下，分区可以有效减少数据读取量，如果服务器较多、硬盘较多，分桶可以更进一步减少数据读取量。
hive的计算速度主要由磁盘io速度决定，增加磁盘和机器可以直观的提升性能。数据分布不均衡会导致计算时间增长。
hive的语法与postgresql稍有不同，主要在时间类型上要进行处理，类型必须对应，pg可以直接使用date类型 >= 字符串 这样的，hive的语法不会报错，但结果会不正确，必须要注意。
hive的数据插入也要注意，尽量直接使用sqoop或java向hdfs中写入数据，使用insert时会转换为mr执行，每插入一条数据都需要10几秒的时间。
早期的hive不支持存储过程，目前的版本可以支持，但存储过程不存于数据库中，需要存成文件，调用时import后使用。


hbase数据存储方式与hive的存储方式不同，占用空间较大，导入hive的515万数据占用了1200M空间，导入hbase的90万数据占用了950M数据。
hive使用sqoop导入数据时，主要用时以读到postgresql的时间为主，写hdfs几乎不占用太多时间，515万数据导入用时不到2分钟，而导入hbase的90万数据用时32分钟。
hbase的单条数据读取只能以key作为条件，读取速度较快。
hbase的key值选择很关键，相当于数据库的主键，如果插入两次相同主键，则认为第二次的插入是update，会覆盖第一条记录。


hive表不能delete，可以truncate。hive的删除和更新都要通过新建表，然后删除原表，再重命名新表来完成。
hbase的数据可以delete，因为delete时只是标记记录为删除状态。



####################################测试cdh5.14.2版本的相关内容#########################
在phoenix中建表slogp，建所有字段的索引。这样在hbase中会有两个表，一个是正常的slogp表，一个是索引表，索引表以slogp表的全字段连接作为主键。数据在插入过程中程序会更新索引。
写了一段程序从pg数据库读取数据，插入phoenix，每千条一提交。
插入100万数据时速度可达3300条/秒
再次插入400万数据时速度平均为1800条/秒
而在最终快插入完成时观察，插入速度低于1000条/秒

在插入过程中观察到机器的cpu及磁盘写入占用都不大，内存会使用较多。插入500万条数据，内存的cache从10G降到4.6G
第一次插入118万数据时发生了一次错误，提示索引更新失败，插入程序报错终止。再次插入400万数据时未发生错误。
!!多次测试都在118万到119万之间报错，应该是跟索引更新机制有关系。
删除全表索引，新建一个只有几个字段的索引，数据插入至224万时报了同样的错。
无索引插入890万数据不会报错。
全索引1万条数据一提交的话，不会报错，可以正常导入900万数据。

##全索引
500万数据：
count 全表用时 1.5秒左右
count 全表带条件用时 2秒左右
group by 全表用时 2秒左右
按条件查询单条数据用时 2秒左右
900万数据：
count 全表用时 第一次执行 15秒， 之后再执行(数据已缓存在内存) 2秒

#每100条一提交
前期速度1580条/秒
导入118万时报错，用时1550秒
#每10000条一提交
780万数据用时2700秒
900万数据用时3050秒

##无索引 查数据时可以观察到有大量磁盘的读取
890万数据：
写入用时2180秒 平均4082条/秒
count 全表用时22秒
count 数据带条件 约13秒到22秒之间 

##局部索引 local index，这个是只在数据所在机器上建立索引，全索引是索引与数据分开存储，索引也是一张数据表，也会平均分布在集群上。local index在hbase中看不到
导入数据，在556万时报出错误Unable to find cached index metadata，错误与全索引所报错误不同。导入停止。
报错后执行count(*)时得出表内数据为2100多万，执行count(userid)后为556万，再次执行count(*) 得出556万。
count 全表用时2秒
重启hbase服务后 count 全表用时4秒


!!删除表数据时出现了问题，由于没有truncate 语法，使用delete from slogp删除，机器的cpu使用达到 100%,时间持续将近4分钟 ，命令行端报超时错误(60秒超时)。
机器的cpu使用率下降后，count一下slogp表，还有281万数据存在。
再次删除后，count表仍然还有950条数据，这950条数据删除不掉。从hbase中查看数据表，slogp中已经没有数据，而索引表中仍有数据。

!!从phoenix中删除数据时，hbase中并没有真正删除数据，只是标记了数据已经删除，仍占用内存和空间，查询所需时间也不会减少。
hbase有truncate语句，可以清除表数据，从hbase中将slogp表数据清除后，再count这张表，hive中会报错，phoenix中不会报错，select * 时hive和phoenix中都不会报错，同时有950条数据。
hbase truncate表只使用了6.5秒。truncate后机器的内存并没有释放。在hive中count表slogp时仍会报错，phoenix中操作正常。

!!!!从hbase中删除数据并不会释放内存，truncate表后，内存仍在使用中，测试时从页面上关闭hbase服务后释放了13G内存。
!!!!hbase在不重启的情况下不会释放内存，内存用完后会用swap，swap没有之后估计就会死机了。



impala的相关测试 impala与hive使用相同的资料库及数据，impala重写了mr过程，而且省略了将语句转换成class的过程，hive需要调用hadoop的mr进行count类计算，而impala则使用自己的计算方法，类似于spark重写mr一样。
hive中建表后需要在impala中执行 INVALIDATE METADATA 后才能读到hive的表。
cdh5.14.2中impala似乎也缓存了数据，count 900万数据第一次用7秒，第二次就只有0.5秒,执行对另一张大表的查询后，再执行这个count 用时13秒。
对4500万的表进行count，第一次65秒，第二次51秒，第三次43秒，第四次12秒，第五次4秒。对另外一张表进行查询后，再count，变为41秒。

在数据量较少的情况下，impala的速度要明显好于hive，当数据量到达一定程度，内存中放不下，需要从hdfs读取时，简单的count因mr过程少hive所用时间接近于impala，对于有条件且group by 的语句，impala还是有明显优势。
大数据量下，hive计算时cpu使用率可达100%，同样语句，impala的cpu使用率在30%以内。
impala对于内存使用也控制得比较好，大数据量下的查询不会无限增加内存使用。
phoenix在数据量较少的情况下速度也比较快，但到达一定程度后，也需要读取hdfs，速度优势相比impala并不明显。
phoenix的数据写入速度很慢，频繁更新数据也会导致索引更新失败。
hbase的内存使用量太大，在服务器内存较小的情况下，大量的写入会使机器崩溃，使用impala基于hbase或phoenix基于hbase都会出现这个问题。
使用impala会有缓存问题，impala默认缓存数据，当程序更新hdfs时，impala(hive)并不知道数据已经更新，从impala查出的内容还是旧内容，需要使用refresh tablename后再查询才能得到正确的数据。如果关闭缓存，impala每次查询都会查询hdfs数据，这样比缓存会慢。
在hive中建立hbase的外部表后，可以在hive中以sql形式查询hbase的数据，impala也可以通过这个外部表查询到hbase的数据，但此种方式并不好，hbase中数据并不分类型，全是字符型，到hive中需要进行转换，部分不符合规则的数据在转换过程中出现错误会导致查询失败或结果不正确。从测试结果来看impala并不会缓存hbase中的数据，impala基于hive的查询在多次执行时时间会逐渐减少，而基于hbase的查询时间一直恒定。

对于像阿里日志这样大量写，少量读的情况，建议使用impala，关闭缓存，数据以天为单位进行分区。
对于大量数据少量写大量读的应用，使用phoenix比较好。




----------------------使用pg测试库中的数据进行的测试---------------------------
导入bi_fdm.d_zxxk_cl_consumelog 表， 共89G数据，用时48分钟。
select count(*) from bi_fdm.d_zxxk_cl_consumelog; 558m 8分钟
建立一个分区表，以consumetime分区，每天一个分区。使用impala插入全部数据会报内存溢出，按年插入用时16分钟。按月插入用时12分钟。
按天查分区表时，速度在6秒内。
#在hive下可使用 set COMPRESSION_CODEC=gzip; 或 set COMPRESSION_CODEC=snappy; 来修改压缩算法，gzip压缩比例高一些，但查询慢。默认的是snappy。
先设置压缩算法再建立一个列存储的压缩表zxc，将所有bi_fdm.d_zxxk_cl_consumelog数据插入用时12分钟。snappy算法zxc占空间30G，压缩比为3:1。gzip点用27G。
select schooluserid , count(*)  from zxc group by schooluserid order by count(*) desc ; snappy用时 5 秒。gzip用时10秒


select count(*) from bi_fdm.d_xxxk_xx_userdownload;小学学科网的下载统计impala用时50秒。
select count(*) from bi_fdm.d_zxxk_cl_consumelog; 中学学科网的下载统计impala行存储用时12分钟。列存储用时6分钟。82的数据库用时110分钟。



----------------------------------kudu对比测试-------------------------
kudu只有主键可以建索引，主键不能更改。数字型的主键有bug，当数据量大一些的话，操作会报超时，string型的主键不会有这个问题。
测试过程中从hive表向kudu表插入数据，impala会报内存不足的错，有时插入500万就报，有时插入700万会成功，可能跟实际使用的内存有关。

select count(*), max(id), min(id) from  bi_fdm.d_pape_usercreatedpaper ;
数据有1966万，数据主要以id存储，用impala直接读取未分区的数据用时14秒，kudu对id进行hash存储读取用时1秒，

查询数据
select count(*), max(id), min(id) from  bi_fdm.kd_d_pape_usercreatedpaper t 
where t.schooluserid > 10000 and t.userid >= 100000;
基于kudu用时4.2秒，基于hive用时16秒

update bi_fdm.kd_d_pape_usercreatedpaper set schooluserid = 100; 90万数据，用时83秒。 

将主键改为string型，
update bi_fdm.kd_d_pape_usercreatedpaper set schooluserid = 100; 1900万数据，更新到800万左右会报错。
delete from  bi_fdm.kd_d_pape_usercreatedpaper where  schooluserid = 100 ; 删除800万数据用时 453秒。
delete from  bi_fdm.kd_d_pape_usercreatedpaper ; 删除1100万数据用时713秒。


插入较大的表 bi_fdm.kd_d_zxxk_cl_consumelog
700万插入 121秒
500万数据id重复，插入时会直接更新，不会报错 351秒
count(*) 全表 5.5亿数据，1.5秒
关联a_res_soft_info表统计每天下载量，执行中报错无法连接另外两台的impala，无法完成计算，重启impala后可以完成计算419秒。使用基于hive的表可以完成，用时12分钟。


--------------------------------parquet hive的列存储组件---------------------------
parquet不需要建主键
在hive中建表 780万数据 用时51秒
create table bi_fdm.pq_d_zxxk_cl_consumelog
    row format serde 'parquet.hive.serde.ParquetHiveSerDe'    
    stored as inputformat 'parquet.hive.DeprecatedParquetInputFormat'                    
    outputformat 'parquet.hive.DeprecatedParquetOutputFormat'
as  SELECT t.* FROM  bi_fdm.d_zxxk_cl_consumelog t where  cdate like  '2016-01%'   ;

使用impala插入
插入2000万数据 18秒
插入1700万数据 15秒
插入5800万数据 51秒 
插入2017年一年的数据 154秒


直接建压缩表 用时2024秒 
create table bi_fdm.pq_d_zxxk_cl_consumelog_snappy
    row format serde 'parquet.hive.serde.ParquetHiveSerDe'    
    stored as inputformat 'parquet.hive.DeprecatedParquetInputFormat'                    
    outputformat 'parquet.hive.DeprecatedParquetOutputFormat'
    TBLPROPERTIES ("parquet.compression"="SNAPPY")
as  SELECT t.* FROM  bi_fdm.d_zxxk_cl_consumelog t 
    ;

建一个空的gzip压缩表 用时8秒
create table bi_fdm.pq_d_zxxk_cl_consumelog_gzip
    row format serde 'parquet.hive.serde.ParquetHiveSerDe'    
    stored as inputformat 'parquet.hive.DeprecatedParquetInputFormat'                    
    outputformat 'parquet.hive.DeprecatedParquetOutputFormat'
    TBLPROPERTIES ("parquet.compression"="GZIP")
as select * from  bi_fdm.d_zxxk_cl_consumelog  where 1=0    ;

再使用impala插入所有数据 用时811秒
insert into bi_fdm.pq_d_zxxk_cl_consumelog_gzip   SELECT  * FROM  bi_fdm.d_zxxk_cl_consumelog ;

未压缩 10秒，4秒
select count(*) from bi_fdm.pq_d_zxxk_cl_consumelog t ;
select count(*),  SUBSTRING(t.cdate, 1, 7) from bi_fdm.pq_d_zxxk_cl_consumelog t group by SUBSTRING(t.cdate, 1, 7) order by  SUBSTRING(t.cdate, 1, 7);

snappy压缩 23秒(这个是先执行的，可能重新读了资料库，所以慢) ， 5秒 
select count(*) from bi_fdm.pq_d_zxxk_cl_consumelog_snappy t ;
select count(*),  SUBSTRING(t.cdate, 1, 7) from bi_fdm.pq_d_zxxk_cl_consumelog_snappy t group by SUBSTRING(t.cdate, 1, 7) order by  SUBSTRING(t.cdate, 1, 7);

gzip压缩 5秒， 5秒
select count(*) from bi_fdm.pq_d_zxxk_cl_consumelog_gzip t ;
select count(*),  SUBSTRING(t.cdate, 1, 7) from bi_fdm.pq_d_zxxk_cl_consumelog_gzip t group by SUBSTRING(t.cdate, 1, 7) order by  SUBSTRING(t.cdate, 1, 7);



测试以下语句，生成所有下载记录，无压缩用时421秒， snappy压缩用时247秒, gzip压缩用时410秒,未使用列存储的表在执行过程中因impala已经调整为不限制内存使用，所用内存过多，swap占用超过90%，impala自动关闭了任务，sql语句执行报错。 重启impala后再执行未使用列存储的表，用时585秒。
create table pqi_no as 
select  from_unixtime(unix_timestamp( c.consumetime) ,"yyyy-MM-dd") stat_date, 
		 		'D' stat_freq,
		 		soft_type,  --	资源类型 作业 课件 教案
			    coalesce(grade_type, '') grade_type , --	年级 一年级 二年级 
			    period_type, --	学段 小学 中学 高中
			    subject_type, --	学科 语文 数学 英语
			    c.schooluserid,
			    count(1) down_num
		   FROM bi_fdm.pq_d_zxxk_cl_consumelog c,
			`default`.a_res_soft_info a
		  where c.infoid = a.softid
		  	and c.schooluserid <> 0
		--  	and c.cdate = '2019-01-01'
		--	and c.consumetime :: DATE  >= public.fc_p_get_date(v_etl_date , 'D')
		--	and c.consumetime :: DATE  <=public.fc_p_get_date(v_etl_date , 'D')  
		group by  from_unixtime(unix_timestamp( c.consumetime) ,"yyyy-MM-dd") ,
				soft_type,  --	资源类型 作业 课件 教案
			    grade_type,  --	年级 一年级 二年级 
			    period_type, --	学段 小学 中学 高中
			    subject_type, --	学科 语文 数学 英语
			    c.schooluserid
		;




-----------------------greenplum--使用4块磁盘的lvm作为硬盘，与hadoop集群使用的硬盘一致------------------------
#使用datax从82机器上导入public.d_zxxk_cl_consumelog(行存储)  ---sqoop导入hive时只用了48分钟，而此次导入用时4个半小时。
#导入过程中82机器的磁盘读取9M/秒，磁盘使用率15%以内，greenplum的磁盘写入60M/秒，磁盘使用率100%，应该是写日志占用了较多的磁盘资源。
All Task WaitWriterTime 8,199.113s |  All Task WaitReaderTime 6,165.959s
2019-08-14 15:42:33.490 [job-0] INFO  JobContainer - 
任务启动时刻                    : 2019-08-14 11:31:18
任务结束时刻                    : 2019-08-14 15:42:33
任务总计耗时                    :              15075s
任务平均流量                    :            4.51MB/s
记录写入速度                    :          37063rec/s
读出记录总数                    :           558539480
读写失败总数                    :                   0

#向列存储表写数据时性能有较大改善，磁盘使用率在20%左右，应该是列存储不用写日志。
All Task WaitWriterTime 20.626s |  All Task WaitReaderTime 5,653.964s | Percentage 100.00%
2019-08-14 19:02:39.733 [job-0] INFO  JobContainer - 
任务启动时刻                    : 2019-08-14 17:26:58
任务结束时刻                    : 2019-08-14 19:02:39
任务总计耗时                    :               5741s
任务平均流量                    :           11.84MB/s
记录写入速度                    :          97306rec/s
读出记录总数                    :           558539480
读写失败总数                    :                   0


#count 行存储数据时磁盘全量读取了数据，用时339秒
select count(*) from public.d_zxxk_cl_consumelog;

#将行存储数据转到列存储中，用时1177秒
create table dcl 
with (appendonly=true      --①仅追加
      ,orientation=column  --②列存储
      ,compresstype=zlib   --③压缩算法
      ,compresslevel=5     --④压缩级别
      ,oids=false)   
as select * from  public.d_zxxk_cl_consumelog; 


#查询列存储的数据，用时22秒 46秒 
select count(*) from public.dcl;
select count(*), date_trunc('month', t.consumetime) from dcl t group by date_trunc('month', t.consumetime) order by  date_trunc('month', t.consumetime);



#计算数据，用时642秒
create table kdxxn with (appendonly=true      --①仅追加
      ,orientation=column  --②列存储      
      ,oids=false) as 
select  c.consumetime::date stat_date, 
		 		'D' stat_freq,
		 		soft_type,  --	资源类型 作业 课件 教案
			    coalesce(grade_type, '') grade_type , --	年级 一年级 二年级 
			    period_type, --	学段 小学 中学 高中
			    subject_type, --	学科 语文 数学 英语
			    c.schooluserid,
			    count(1) down_num
		   FROM dcl  c,
		  --from zxc c,
				a_res_soft_info a
		  where c.infoid = a.softid
		  	and c.schooluserid <> 0
		--  	and c.cdate = '2019-01-01'
		--	and c.consumetime :: DATE  >= public.fc_p_get_date(v_etl_date , 'D')
		--	and c.consumetime :: DATE  <=public.fc_p_get_date(v_etl_date , 'D')  
		group by  c.consumetime::date,
				soft_type,  --	资源类型 作业 课件 教案
			    grade_type,  --	年级 一年级 二年级 
			    period_type, --	学段 小学 中学 高中
			    subject_type, --	学科 语文 数学 英语
			    c.schooluserid
		;
		







