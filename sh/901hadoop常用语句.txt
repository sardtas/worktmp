
安装sdk
curl -s "https://get.sdkman.io" | bash


看端口状态
 netstat -anp |grep 10000



------------------------------------------kafka:

kafka-topics.sh --create --zookeeper datanode1:2181 --replication-factor 1 --partitions 3 --topic  tp1_d_log
kafka-topics.sh --list --zookeeper datanode1:2181
kafka-topics.sh  --delete --zookeeper datanode1:2181  --topic tp1_d_log

kafka-console-producer.sh --broker-list datanode1:9092 --topic tp1_d_log
kafka-console-consumer.sh --zookeeper datanode1:2181 --topic tp1_d_log --from-beginning


1.启动主节点nimbus ：
storm nimbus >/dev/null 2>&1 & 
1
2.启动两个从节点supervisor：
storm supervisor >/dev/null 2>&1 &
1
3.主节点上启动UI管理
storm ui >/dev/null 2>&1 & 
--------------------- 


注意环境变量是否配置成功
kafka-server-start.sh config/server.properties &
#后台启动
kafka-server-start.sh config/server.properties 1>/dev/null 2>&1 &
#如没有配置环境变量也可以直接使用绝对路径（或相对路径）启动
/usr/local/kafka_2.11-0.10.2.1/bin/kafka-server-start.sh /usr/local/kafka_2.11-0.10.2.1/config/server.properties &



---------------------------------------zookeeper
sh ./bin/zkServer.sh start
1
使用客户端链接zookeeper

bin/zkCli.sh -server 192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181





----------------------------------------hadoop
yarn 启动不了 报local-dirs are bad: 
空间不够90%，就会报这个错

----------------------------------------spark
编译spark
mvn -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.7 -Phadoop-provided -Phive -Phive-thriftserver -Pnetlib-lgpl -DskipTests clean package
mvn -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.7 -Phadoop-provided -Pnetlib-lgpl -DskipTests clean package
build/mvn -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.7 -DskipTests clean package
用下边这个比较好
./dev/make-distribution.sh --name hadoop2.7.7 --tgz  -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.7

---------------------------------------hive
编译hive
使用 hive-2.3.4版本，其他可能都不好用
只有这名好用：
mvn clean package -Pdist -DskipTests
以下的都不好用：
mvn clean install -Phadoop-2 -DskipTests -Dhadoop.version=2.7.7 -Dspark.version=2.3.3 
mvn clean -Phadoop-2.7 -DskipTests -Dhadoop.version=2.7.7 -Dspark.version=2.3.3 
git clone https://git-wip-us.apache.org/repos/asf/hive.git


hive on spark
修改bin/hive，增加spark的包，以下语句
for f in ${SPARK_HOME}/jars/*.jar; do
     CLASSPATH=${CLASSPATH}:$f;
done

hive 
初始化数据库
schematool -dbType postgres -initSchema

将$SPARK_HOME/lib目录下面的spark-assembly开头的那个jar包拷贝到$HIVE_HOME/lib目录下面,否则报找不到类的错误

schematool -dbType postgres -initSchema
要在hadoop core-site.xml中增加以下内容使hive能访问hadoop数据，其中yy要换成安装用户名
<property>
    <name>hadoop.proxyuser.yy.hosts</name>
    <value>*</value>
</property>
<property>
    <name>hadoop.proxyuser.yy.groups</name>
    <value>*</value>
</property>

==========

<property>
<name>hive.enable.spark.execution.engine</name>
<value>true</value>
</property>
<property>
<name>spark.home</name>
<value>/data/hadoop/spark</value>
</property>
<property>
<name>spark.enentLog.enabled</name>
<value>true</value>
</property>
<property>
<name>spark.enentLog.dir</name>
<value>hdfs://nn1:9000/spark-logs</value>
</property>
<property>
<name>spark.serializer</name>
<value>org.apache.spark.serializer.KryoSerializer</value>
</property>
<property>
<name>spark.executor.extraJavaOptions</name>
<value>-XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"</value>
</property>



=========
改hive引擎 
set hive.execution.engine=spark;
set hive.execution.engine=mr;

=========
yarn报return 1 没有权限的问题 以下可能不是主要问题，应该是yarn问题。
groupadd supergroup
usermod -a -G supergroup root
usermod -a -G supergroup hive
usermod -a -G supergroup yy

usermod -a -G supergroup mapred
usermod -a -G supergroup hdfs
usermod -a -G supergroup hive
usermod -a -G supergroup hue
usermod -a -G supergroup spark
sudo -u hdfs hadoop fs -chmod 770 /user

数据太多导不进去

----------------------------------------sqoop

sqoop import   \
--connect jdbc:postgresql://10.1.1.97:5432/dc_logs --username bi_log --password bi_log123456 -m 1 \
--table d_zxxk_cl_soft  --fields-terminated-by '\001' --append --direct --where ' softid < 10000000' \
--target-dir /user/hive/warehouse/t25 -- --schema bi_fdm 

sqoop import   \
--connect jdbc:postgresql://10.1.1.97:25432/dc_logs --username bi_log --password uat123 -m 1 \
--table c_cfg_glob_para --direct --fields-terminated-by '\001' \
--target-dir /user/hive/warehouse/bi_log.db/t31 -- --schema bi_fdm 

hadoop的mr任务一直在等待，状态是accept，但是一直不运行，因为yarn认为没有资源可以运行。在yarn-site.xml中增加以下内容
一直等待是因为datanode2和3返回信息的时候使用了自己的机器名，datanode1找不到机器名

<property><name>yarn.scheduler.minimum-allocation-mb</name> 
	<value>1024</value> <discription>单个任务可申请最少内存，默认1024MB</discription> </property> 

<property> <name>yarn.nodemanager.resource.memory-mb</name>
 <value>4096</value> <discription>nodemanager默认内存大小，默认为8192MB（value单位为MB）</discription> </property> 

<property> <name>yarn.nodemanager.resource.cpu-vcores</name> 
<value>4</value> <discription>nodemanager cpu内核数</discription> </property>

<property> <name>yarn.scheduler.minimum-allocation-mb</name>
<value>1024</value> </property>

增加机器名试试
172.17.0.6 9c904bfe588d
172.17.0.7 578fdabd628c
172.17.0.5 01473e201d47
172.17.0.5      datanode1
172.17.0.6      datanode2
172.17.0.7      datanode3
-------------------------------------------------





















=======================================
idea的认证服务器
docker run --name idea -p 1017:1017 -e USER="yangyong" -d dominate/idea-license-server


hive编译
mvn clean package -Pdist
mvn clean package -Pdist -DskipTests
spark编译 
build/mvn clean package -Pdist -DskipTests

ftp使用
apt install vsftpd
vim /etc/vsftpd.conf
listen=YES
local_enable=YES
local_root=/work
write_enable=YES




---------------------------------------
linux相关
ubuntu 禁用服务
update-rc.d apache2 remove
redhat centos 禁用服务
chkconfig apache2 off







